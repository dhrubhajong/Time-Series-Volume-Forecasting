{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Define an empty DataFrame\n",
    "data = pd.DataFrame(columns=['date', 'team', 'request_type', 'team_request_type', 'volume'])\n",
    "\n",
    "# Define date range with a monthly frequency\n",
    "date_range = pd.date_range(start=\"2020-01-01\", end=\"2023-08-01\", freq='MS')\n",
    "\n",
    "# Create a list of unique teams and request types\n",
    "teams = [\"Team A\", \"Team B\", \"Team C\", \"Team D\", \"Team E\"]\n",
    "request_types = [f\"Request {i}\" for i in range(1, 21)]\n",
    "\n",
    "# Generate data for each month from 2020 to 2023\n",
    "for date in date_range:\n",
    "    for team in teams:\n",
    "        for request_type in request_types:\n",
    "            team_request_type = f\"{team} - {request_type}\"\n",
    "            volume = random.randint(10, 1000)\n",
    "\n",
    "            data = data.append({\n",
    "                'date': date,\n",
    "                'team': team,\n",
    "                'request_type': request_type,\n",
    "                'team_request_type': team_request_type,\n",
    "                'volume': volume\n",
    "            }, ignore_index=True)\n",
    "\n",
    "# Sort the DataFrame by date\n",
    "data = data.sort_values(by='date')\n",
    "data.to_csv('mock_data.csv', index=False)  # Save the data to a CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io \n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.graphics.tsaplots as sgt \n",
    "import statsmodels.tsa.stattools as sts \n",
    "from statsmodels.tsa.seasonal import seasonal_decompose \n",
    "from scipy.stats.distributions import chi2\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'data'\n",
    "os.makedirs(output_dir, exist_ok = True)\n",
    "\n",
    "input_dir = 'data'\n",
    "file = 'mock_data.csv'\n",
    "\n",
    "output_file = 'output_file.xlsb'\n",
    "output_file_path = os.path.join(output_dir, output_file)\n",
    "\n",
    "\n",
    "output_excel_dir = 'results_'\n",
    "os.makedirs(output_excel_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "# Function to extract the year from the filename\n",
    "def extract_year_from_filename(filename):\n",
    "    return filename.split('_')[-1].split('.')[0]\n",
    "\n",
    "# Initialize an empty DataFrame to store merged data\n",
    "merged_data = pd.DataFrame()\n",
    "\n",
    "# Define a dictionary to map month names to numerical values\n",
    "month_mapping = {\n",
    "    'January': '01', 'February': '02', 'March': '03', 'April': '04',\n",
    "    'May': '05', 'June': '06', 'July': '07', 'August': '08',\n",
    "    'September': '09', 'October': '10', 'November': '11', 'December': '12'\n",
    "}\n",
    "\n",
    "# Loop through Excel files\n",
    "for input_file_path in excel_file_paths:\n",
    "    # Open the Excel workbook with open_workbook(input_file_path) as wb:\n",
    "    with open_workbook(input_file_path) as wb:\n",
    "        for sheet_name in wb.sheets:\n",
    "            year = extract_year_from_filename(sheet_name)\n",
    "\n",
    "            # Initialize empty lists to store data and column names\n",
    "            data = []\n",
    "            column_names = []\n",
    "\n",
    "            # Get the sheet by name\n",
    "            with wb.get_sheet(sheet_name) as sheet:\n",
    "                for row_num, row in enumerate(sheet.rows()):\n",
    "                    row_data = [item.v for item in row]\n",
    "                    data.append(row_data)\n",
    "\n",
    "                    if row_num == 0:\n",
    "                        column_names = row_data\n",
    "\n",
    "                # Create a DataFrame with the data and column names\n",
    "                df = pd.DataFrame(data, columns=column_names)\n",
    "                df = df.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "                # Melt the DataFrame to transpose it\n",
    "                df_melted = df.melt(id_vars=['team', 'request type'], var_name='Month', value_name='volume')\n",
    "                df_melted['Month'] = df_melted['Month'].map(month_mapping)\n",
    "                df_melted['date'] = year + '-' + df_melted['Month'] + '-01'\n",
    "                df_melted = df_melted[['date', 'Month', 'Team', 'request type', 'volume']\n",
    "\n",
    "                # Replace blank/NaN with 0\n",
    "                df_melted['Value'] = df_melted['Value'].fillna(0)\n",
    "                df_melted = df_melted.groupby(['Date', 'Month', 'Team', 'Request Type'], as_index=False)['Value'].sum()\n",
    "                merged_data = pd.concat([merged_data, df_melted], ignore_index=True)\n",
    "                merged_data['Year'] = merged_data['Date'].str.split('-').str[0]\n",
    "                column_order = ['Date', 'Month', 'Year', 'Team', 'Request Type', 'Value']\n",
    "                merged_data = merged_data[column_order]\n",
    "\n",
    "                merged_output_file_path = os.path.join(output_data_directory, 'merged_transposed_data.xlsx')\n",
    "                merged_data.to_excel(merged_output_file_path, index=False)\n",
    "\n",
    "print(f'Merged file saved in folder \"{output_data_directory}\": {merged_output_file_path}')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "# Function to read data from a file\n",
    "def read_data(file_path):\n",
    "    return pd.read_excel(file_path)\n",
    "\n",
    "# Function to clean data\n",
    "def clean_data(df):\n",
    "    df.dropna(subset=['request type', 'volume'], how='all', inplace=True)\n",
    "    \n",
    "    # Convert the 'Date' column to datetime format\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Function to transform data\n",
    "def transform_data(df):\n",
    "    df = df.groupby(['date', 'Month', 'year', 'team', 'request type'], as_index=False)['volume'].sum()\n",
    "    \n",
    "    # Create a new column 'Team_Request_Type' by combining 'Team' and 'Request Type'\n",
    "    df['team_request_type'] = df['team'] + ' - ' + df['request Type']\n",
    "    \n",
    "    column_order = ['date', 'Month', 'year', 'team', 'request Type', 'team_request_type', 'volume']\n",
    "    df = df[column_order]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Function to clean and transform data\n",
    "def clean_and_transform(input_file_path):\n",
    "    df = read_data(input_file_path)\n",
    "    clean_data(df)\n",
    "    df = transform_data(df)\n",
    "    return df\n",
    "\n",
    "# Define input file path\n",
    "input_data_directory = \"data\"\n",
    "file = \"merged_transposed_data.xlsx\"\n",
    "input_file_path = os.path.join(input_data_directory, file)\n",
    "\n",
    "# Clean and transform the data\n",
    "cleaned_and_transformed_data = clean_and_transform(input_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file using pandas\n",
    "csv_file_path = os.path.join(input_dir, file)\n",
    "df = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the year from a date\n",
    "def extract_year(date_str):\n",
    "    # Split the date string by '-' and get the first element (the year)\n",
    "    return date_str.split('-')[0]\n",
    "\n",
    "# Apply the function to create a new 'Year' column\n",
    "df['Year'] = df['date'].apply(lambda x: extract_year(x))\n",
    "df['Year'] = df['Year'].astype(int)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "end_year = df['Year'].max()\n",
    "start_year = end_year - 3 \n",
    "filtered_data = df[df['Year'] >= start_year]\n",
    "\n",
    "request_team_type_counts = filtered_data['team_request_type'].value_counts().reset_index()\n",
    "request_team_type_counts.columns = ['team_request_type', 'count']  # Make sure to use consistent column names\n",
    "\n",
    "team_request_type_44_months = request_team_type_counts[request_team_type_counts['count'] == 44]\n",
    "total_team_request_type_44_months_count = len(team_request_type_44_months)\n",
    "\n",
    "print(\"Total 'team_request_type' values with 44 months of data:\", total_team_request_type_44_months_count)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cleaned_and_transformed_data.columns.tolist()\n",
    "selected_columns = ['date', 'Month', 'year', 'team', 'request type', 'request_team_type', 'volume']\n",
    "cleaned_and_transformed_data['request_team_type'] = cleaned_and_transformed_data['request_team_type']\n",
    "\n",
    "merged_data = pd.merge(team_request_type_44_months, cleaned_and_transformed_data['selected_columns'],\n",
    "                       on='Team_Request_Type',\n",
    "                       how='inner')\n",
    "\n",
    "desired_columns = ['date', 'Month', 'year', 'team', 'request_team_type', 'volume']\n",
    "df = df[desired_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the date\n",
    "df = df.sort_values(by = ['team_request_type', 'date'])                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "# merged_data.index.freq = 'm'\n",
    "# merged_data.set_index(\"date\", inplace=true)\n",
    "# convert date strings to datetime objects\n",
    "\n",
    "train_start_date = pd.to_datetime('2020-01-01')\n",
    "train_end_date = pd.to_datetime('2023-05-01')\n",
    "\n",
    "valid_start_date = pd.to_datetime('2023-06-01')\n",
    "valid_end_date = pd.to_datetime('2023-08-01')\n",
    "\n",
    "\n",
    "train = df[(df['date'] >= train_start_date) & (df['date'] <= train_end_date)]\n",
    "valid = df[(df['date'] >= valid_start_date) & (df['date'] <= valid_end_date)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_exclude = ['Year']\n",
    "train = train.drop(columns = col_to_exclude)\n",
    "valid = valid.drop(columns = col_to_exclude)\n",
    "\n",
    "df.rename(columns = {'date':'ds' , 'team_request_type':'unique_id' , 'volume':'y'}, inplace = True) \n",
    "\n",
    "train.rename(columns = {'date':'ds' , 'team_request_type':'unique_id' , 'volume':'y'}, inplace = True) \n",
    "valid.rename(columns = {'date':'ds' , 'team_request_type':'unique_id' , 'volume':'y'}, inplace = True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holtwinters Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " best model mae for Team A - Request 1 : 87.52582637770672\n",
      " best model mae for Team A - Request 10 : 194.31770855882863\n",
      " best model mae for Team A - Request 11 : 600.9412077947063\n",
      " best model mae for Team A - Request 12 : 74.65120275673219\n",
      " best model mae for Team A - Request 13 : 250.0985676207357\n",
      " best model mae for Team A - Request 14 : 405.35705096448834\n",
      " best model mae for Team A - Request 15 : 483.09841370769954\n",
      " best model mae for Team A - Request 16 : 472.68055344571576\n",
      " best model mae for Team A - Request 17 : 381.52689738687104\n",
      " best model mae for Team A - Request 18 : 504.0960474980002\n",
      " best model mae for Team A - Request 19 : 122.17912430772769\n",
      " best model mae for Team A - Request 2 : 599.6155374912984\n",
      " best model mae for Team A - Request 20 : 200.1102496666667\n",
      " best model mae for Team A - Request 3 : 206.58344018310459\n",
      " best model mae for Team A - Request 4 : 322.4664834079936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages/statsmodels/tsa/holtwinters/model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " best model mae for Team A - Request 5 : 138.1504185145815\n",
      " best model mae for Team A - Request 6 : 411.6989488428112\n",
      " best model mae for Team A - Request 7 : 670.7749121098907\n",
      " best model mae for Team A - Request 8 : 149.29258529098396\n",
      " best model mae for Team A - Request 9 : 271.54318225541994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/anaconda-2022.05-py39/lib/python3.9/site-packages/statsmodels/tsa/holtwinters/model.py:915: ConvergenceWarning: Optimization failed to converge. Check mle_retvals.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " best model mae for Team B - Request 1 : 146.19390042542656\n",
      " best model mae for Team B - Request 10 : 822.0070023989991\n",
      " best model mae for Team B - Request 11 : 567.1779043602642\n",
      " best model mae for Team B - Request 12 : 235.8164093478881\n",
      " best model mae for Team B - Request 13 : 320.66687696419655\n",
      " best model mae for Team B - Request 14 : 307.032468025568\n",
      " best model mae for Team B - Request 15 : 356.4658919138288\n",
      " best model mae for Team B - Request 16 : 108.21014252591199\n",
      " best model mae for Team B - Request 17 : 262.84926375658654\n",
      " best model mae for Team B - Request 18 : 484.88427646640986\n"
     ]
    }
   ],
   "source": [
    "mae_df = pd.DataFrame(columns = ['unique_id' , 'MAE'])\n",
    "\n",
    "unique_ids = train['unique_id'].unique()\n",
    "result_dict = {}\n",
    "\n",
    "for target_unique_id in unique_ids:\n",
    "    train_subset = train[train['unique_id']==target_unique_id]\n",
    "    valid_subset = valid[valid['unique_id']==target_unique_id]\n",
    "    \n",
    "    train_subset['ds'] = pd.to_datetime(train_subset['ds'])\n",
    "    train_subset.set_index(pd.DatetimeIndex(train_subset['ds'], freq = 'MS'),inplace = True)\n",
    "    \n",
    "    valid_subset['ds'] = pd.to_datetime(valid_subset['ds'])\n",
    "    valid_subset.set_index(pd.DatetimeIndex(valid_subset['ds'], freq = 'MS'),inplace = True)\n",
    "    \n",
    "    \n",
    "#     df['ds'] = pd.to_datetime(df['ds'])\n",
    "#     df.set_index(pd.DatetimeIndex(df['ds'], freq = 'MS'),inplace = True)\n",
    "    \n",
    "    \n",
    "    # Capping in 95th & 5th percentile value\n",
    "    \n",
    "    p95 = np.percentile(train_subset['y'],95)\n",
    "    p5 = np.percentile(train_subset['y'],5)\n",
    "    \n",
    "    train_subset['y'] = np.where(train_subset['y'] > p95,p95, train_subset['y'])\n",
    "    train_subset['y'] = np.where(train_subset['y'] > p5,p5, train_subset['y'])\n",
    "    \n",
    "    # define hyper parameters \n",
    "    alphas = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "    betas = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "    gammas = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "    seasonal_periods = [12]\n",
    "    \n",
    "    best_mae = float('inf')\n",
    "    best_params = None\n",
    "    \n",
    "    for trend_type in ['add','mul']:\n",
    "        for alpha in alphas:\n",
    "            for beta in betas:\n",
    "                for gamma in gammas:\n",
    "                    for seasonal_period in seasonal_periods:\n",
    "                        model = sm.tsa.ExponentialSmoothing(train_subset['y'], trend = trend_type, seasonal = 'add' , seasonal_periods = seasonal_period)\n",
    "                        model_fit = model.fit(smoothing_level = alpha, smoothing_slope = beta, smoothing_seasonal = gamma , optimized = False)\n",
    "                        \n",
    "                        forecast = model_fit.forecast(len(valid_subset))\n",
    "                        \n",
    "                        mae = mean_absolute_error(valid_subset['y'], forecast)\n",
    "                        \n",
    "                        if mae < best_mae:\n",
    "                            best_mae = mae\n",
    "                            best_params =  (trend_type, alpha, beta, gamma, seasonal_period)\n",
    "                            \n",
    "                        mae_df = mae_df.append({'unique_id':target_unique_id, 'MAE': best_mae},ignore_index = True)\n",
    "                        \n",
    "    best_trend, best_alpha, best_beta, best_gamma, best_seasonal_period = best_params \n",
    "    \n",
    "    # refit the best model with best hyper parameters\n",
    "    best_model = sm.tsa.ExponentialSmoothing(train_subset['y'], trend=  best_trend, seasonal=  'add', seasonal_periods=best_seasonal_period)\n",
    "    best_model_fit = best_model.fit(smoothing_level = best_alpha, smoothing_slope = best_beta, smoothing_seasonal = best_gamma )\n",
    "\n",
    "    future_forecasts = best_model_fit.forecast(steps = 6) # forecasting months\n",
    "    \n",
    "    forecast_dates = pd.date_range(start = valid_subset.index[-1], periods = 6, freq = 'MS')\n",
    "    future_forecast_df = pd.DataFrame({'ds': forecast_dates, 'forecasted_value': future_forecasts})\n",
    "    \n",
    "    #plot the charts\n",
    "    plt.figure(figsize = (12,6))\n",
    "    plt.plot (train_subset.index, train_subset['y'], label = 'Training data' , linestyle = '--', marker = 'o')\n",
    "    plt.plot(future_forecast_df['ds'], future_forecast_df['forecasted_value'], label='forecast_volume', linestyle='--', marker='o', color='green')\n",
    "    plt.xlabel('date')\n",
    "    plt.ylabel('volume')\n",
    "    plt.legend()\n",
    "    plt.title(f\"unique ID: {target_unique_id} - {best_trend} Time Series Forecast Comparison\")\n",
    "    \n",
    "    print(f\" best model mae for {target_unique_id} : {best_mae}\")\n",
    "    \n",
    "    \n",
    "    # Create a dictionary to store actual vs. predicted values for each unique ID\n",
    "    result_dict = {}\n",
    "\n",
    "# Loop through unique IDs\n",
    "    for target_unique_id in unique_ids:\n",
    "        # Extract actual and predicted values for the current unique ID\n",
    "        actual_values = valid_subset['y'].tolist()\n",
    "        predicted_values = future_forecasts.tolist()\n",
    "\n",
    "        # Find the maximum length of actual and predicted values\n",
    "        max_len = max(len(actual_values), len(predicted_values))\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "        # Iterate through the data points\n",
    "        for i in range(max_len):\n",
    "            actual_value = actual_values[i] if i < len(actual_values) else None\n",
    "            predicted_value = predicted_values[i] if i < len(predicted_values) else None\n",
    "\n",
    "            # Create a dictionary for the data point\n",
    "            data_list.append({\n",
    "                'ds': i + 1,\n",
    "                'unique_id': target_unique_id,\n",
    "                'actual_values': actual_value,\n",
    "                'predicted_values': predicted_value\n",
    "            })\n",
    "\n",
    "        # Create a DataFrame from the data list\n",
    "        df = pd.DataFrame(data_list)\n",
    "\n",
    "        # Define the output Excel file path\n",
    "        output_excel_file = os.path.join(output_excel_dir, f'{target_unique_id}_forecast_results.xlsx')\n",
    "\n",
    "        # Save the DataFrame to an Excel file\n",
    "        df.to_excel(output_excel_file, index=False)\n",
    "\n",
    "        # Store the actual vs. predicted values in the result dictionary\n",
    "        result_dict[target_unique_id] = {\n",
    "            'actual_values': actual_values,\n",
    "            'predicted_values': predicted_values\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2022.05-py39",
   "language": "python",
   "name": "conda-env-anaconda-2022.05-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
